import os
import glob
import random
from pathlib import Path
import argparse
import h5py
import math
import numpy as np
import pandas as pd
import geopandas
from shapely.geometry import Polygon, box
import rasterio
from rasterio.windows import from_bounds
from pyproj import Transformer, CRS
from tqdm import tqdm
import matplotlib.pyplot as plt

# --- Configuration ---
BASE_DATA_DIR = Path("data")
PREPROCESSED_DATA_DIR = Path("preprocessed_data")
VISUALIZATION_DIR = PREPROCESSED_DATA_DIR / "visualizations"
ROI_FILE_NAME = "roi.txt"
WGS84_CRS = "EPSG:4326"
RANDOM_SEED = 42 # For reproducible splits

PATCH_WIDTH_PIXELS = 256
PATCH_HEIGHT_PIXELS = 256
NUM_VISUALIZATIONS_PER_SPLIT = 10

def get_vegetation_columns(df: pd.DataFrame) -> list:
    """Identifies the correct vegetation column names from the dataframe."""
    try:
        start_index = df.columns.to_list().index('Ste')
        excluded_cols = {'Dens_veg', 'Mat_Pos', 'Comments', 'Kontroll%'}
        potential_cols = df.columns[start_index:].to_list()
        veg_cols = [
            col for col in potential_cols 
            if col not in excluded_cols and 'Unnamed' not in str(col)
        ]
        return veg_cols
    except ValueError:
        raise ValueError("Critical error: Column 'Ste' not found in the Excel file.")

def find_data_pairs(data_dir: Path) -> list[tuple[Path, Path]]:
    """Finds matching pairs of GeoTIFF and XLSX files."""
    pairs = []
    geotiff_files = glob.glob(str(data_dir / "**" / "*.tif"), recursive=True)
    for tif_path_str in geotiff_files:
        tif_path = Path(tif_path_str)
        potential_xlsx_files = list(tif_path.parent.glob(f"{tif_path.stem}*.xlsx"))
        if potential_xlsx_files:
            pairs.append((tif_path, potential_xlsx_files[0]))
        else:
            print(f"Warning: No matching XLSX file found for {tif_path.name}")
    return pairs

def load_roi_polygon_wgs84(data_dir: Path) -> Polygon:
    """Loads ROI points from roi.txt and returns a Polygon in WGS84 CRS."""
    roi_filepath = data_dir / ROI_FILE_NAME
    if not roi_filepath.exists():
        raise FileNotFoundError(f"{ROI_FILE_NAME} not found in {data_dir}. Cannot proceed.")
    
    points_wgs84 = pd.read_csv(roi_filepath, header=None, names=['lat', 'lon'])
    return Polygon(zip(points_wgs84.lon, points_wgs84.lat))

def get_roi_overlapping_files(data_pairs: list, roi_poly_wgs84: Polygon) -> list:
    """Filters the list of data pairs to include only those whose GeoTIFF bounds intersect the ROI."""
    overlapping_files = []
    for tif_path, xlsx_path in tqdm(data_pairs, desc="Checking for ROI overlap"):
        try:
            with rasterio.open(tif_path) as src:
                # Create a polygon for the raster's bounds
                raster_bounds_poly = box(*src.bounds)
                # Define a transformer from WGS84 to the raster's CRS
                transformer = Transformer.from_crs(WGS84_CRS, src.crs, always_xy=True)
                # Transform the ROI polygon to match the raster's CRS
                transformed_roi_poly = Polygon([transformer.transform(x, y) for x, y in roi_poly_wgs84.exterior.coords])
                
                # If the raster's bounds intersect the ROI, keep the file pair
                if raster_bounds_poly.intersects(transformed_roi_poly):
                    overlapping_files.append((tif_path, xlsx_path))
        except Exception as e:
            print(f"Warning: Could not process {tif_path.name} for ROI check. Error: {e}")
    return overlapping_files

def process_data_pair(tif_path: Path, xlsx_path: Path, roi_polygon: Polygon, output_dir: Path, generated_patches: list):
    """Processes a tif/xlsx pair to generate patches based on annotations within the ROI."""
    try:
        with rasterio.open(tif_path) as src:
            source_crs = src.crs
            df = pd.read_excel(xlsx_path, engine='openpyxl')
            if 'Latitud' not in df.columns or 'Longitud' not in df.columns: return

            gdf = geopandas.GeoDataFrame(
                df, 
                geometry=geopandas.points_from_xy(df.Longitud, df.Latitud),
                crs=WGS84_CRS
            ).to_crs(source_crs)

            gdf_filtered = gdf[gdf.geometry.within(roi_polygon)].copy()
            if gdf_filtered.empty: return
            
            veg_cols = get_vegetation_columns(gdf_filtered)
            gdf_filtered[veg_cols] = gdf_filtered[veg_cols].apply(pd.to_numeric, errors='coerce').fillna(0)
            
            patch_count = 0
            for _, annotation in gdf_filtered.iterrows():
                patch_count += 1
                
                px, py = annotation.geometry.x, annotation.geometry.y
                pixel_width_m, pixel_height_m = src.transform.a, -src.transform.e
                patch_width_m, patch_height_m = PATCH_WIDTH_PIXELS * pixel_width_m, PATCH_HEIGHT_PIXELS * pixel_height_m

                offset_x, offset_y = random.uniform(-patch_width_m / 2, patch_width_m / 2), random.uniform(-patch_height_m / 2, patch_height_m / 2)
                center_x, center_y = px - offset_x, py - offset_y
                minx, maxx, miny, maxy = center_x - patch_width_m/2, center_x + patch_width_m/2, center_y - patch_height_m/2, center_y + patch_height_m/2
                
                try:
                    patch_window = from_bounds(minx, miny, maxx, maxy, src.transform)
                    source_patch = src.read(window=patch_window)
                    _, actual_height, actual_width = source_patch.shape
                    if actual_height != PATCH_HEIGHT_PIXELS or actual_width != PATCH_WIDTH_PIXELS:
                        padded_patch = np.zeros((src.count, PATCH_HEIGHT_PIXELS, PATCH_WIDTH_PIXELS), dtype=source_patch.dtype)
                        padded_patch[:, :actual_height, :actual_width] = source_patch
                        source_patch = padded_patch
                except Exception: continue

                patch_bounds_poly = Polygon.from_bounds(minx, miny, maxx, maxy)
                annotations_in_patch = gdf_filtered[gdf_filtered.geometry.within(patch_bounds_poly)]
                if annotations_in_patch.empty: continue

                num_species = len(veg_cols)
                target_binary = np.zeros((PATCH_HEIGHT_PIXELS, PATCH_WIDTH_PIXELS), dtype=np.uint8)
                target_binary_multi = np.zeros((num_species, PATCH_HEIGHT_PIXELS, PATCH_WIDTH_PIXELS), dtype=np.uint8)
                target_continuous_multi = np.zeros((num_species, PATCH_HEIGHT_PIXELS, PATCH_WIDTH_PIXELS), dtype=np.float32)

                for _, ann_in_patch in annotations_in_patch.iterrows():
                    col, row = int((ann_in_patch.geometry.x - minx) / pixel_width_m), int((maxy - ann_in_patch.geometry.y) / pixel_height_m)
                    if not (0 <= row < PATCH_HEIGHT_PIXELS and 0 <= col < PATCH_WIDTH_PIXELS): continue

                    if ann_in_patch[veg_cols].sum() >= 40: target_binary[row, col] = 1
                    for i, col_name in enumerate(veg_cols):
                        species_value = ann_in_patch[col_name]
                        if species_value >= 10: target_binary_multi[i, row, col] = 1
                        target_continuous_multi[i, row, col] = species_value * 0.01
                
                base_name = f"{tif_path.stem}_patch_{patch_count:04d}"
                output_dir.mkdir(parents=True, exist_ok=True)
                with h5py.File(output_dir / f"{base_name}_source.h5", "w") as hf: hf.create_dataset("image", data=source_patch)
                with h5py.File(output_dir / f"{base_name}_target_binary.h5", "w") as hf: hf.create_dataset("target", data=target_binary)
                with h5py.File(output_dir / f"{base_name}_target_binary_multi.h5", "w") as hf: hf.create_dataset("target", data=target_binary_multi)
                with h5py.File(output_dir / f"{base_name}_target_continuous_multi.h5", "w") as hf: hf.create_dataset("target", data=target_continuous_multi)
                
                generated_patches.append({'base_name': base_name, 'veg_cols': veg_cols, 'output_dir': output_dir})

    except Exception as e:
        print(f"FATAL: Failed to process pair {tif_path.name}, {xlsx_path.name}. Error: {e}")

def create_visualization(patch_info: dict, vis_dir: Path):
    """Creates and saves a visualization for a single data patch."""
    base_name, veg_cols, data_dir = patch_info['base_name'], patch_info['veg_cols'], patch_info['output_dir']
    num_species = len(veg_cols)
    
    try:
        with h5py.File(data_dir / f"{base_name}_source.h5", 'r') as hf: source_img = hf['image'][:]
        with h5py.File(data_dir / f"{base_name}_target_binary.h5", 'r') as hf: target_bin = hf['target'][:]
        with h5py.File(data_dir / f"{base_name}_target_binary_multi.h5", 'r') as hf: target_bin_multi = hf['target'][:]
        with h5py.File(data_dir / f"{base_name}_target_continuous_multi.h5", 'r') as hf: target_cont_multi = hf['target'][:]

        rgb_img = source_img[:3, :, :]
        for i in range(rgb_img.shape[0]):
            min_val, max_val = np.min(rgb_img[i]), np.max(rgb_img[i])
            if max_val > min_val: rgb_img[i] = (rgb_img[i] - min_val) / (max_val - min_val)
        rgb_img = np.transpose(rgb_img, (1, 2, 0))

        total_plots = 1 + 1 + num_species + num_species
        cols, rows = 4, math.ceil(total_plots / 4)
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))
        axes = axes.flatten()

        axes[0].imshow(rgb_img); axes[0].set_title("Source RGB"); axes[0].axis('off')
        axes[1].imshow(target_bin, cmap='magma', vmin=0, vmax=1); axes[1].set_title("Binary Target (Veg >= 40%)"); axes[1].axis('off')

        plot_idx = 2
        for i in range(num_species):
            axes[plot_idx].imshow(target_bin_multi[i,:,:], cmap='magma', vmin=0, vmax=1); axes[plot_idx].set_title(f"Bin Multi - L{i}: {veg_cols[i]}"); axes[plot_idx].axis('off'); plot_idx += 1
        for i in range(num_species):
            im = axes[plot_idx].imshow(target_cont_multi[i,:,:], cmap='viridis', vmin=0, vmax=1); axes[plot_idx].set_title(f"Cont Multi - L{i}: {veg_cols[i]}"); axes[plot_idx].axis('off'); fig.colorbar(im, ax=axes[plot_idx]); plot_idx += 1
        
        for i in range(plot_idx, len(axes)): axes[i].axis('off')

        plt.tight_layout()
        vis_dir.mkdir(parents=True, exist_ok=True)
        plt.savefig(vis_dir / f"{base_name}.png", dpi=150)
        plt.close(fig)

    except Exception as e:
        print(f"    - Could not create visualization for {base_name}. Error: {e}")

def main():
    parser = argparse.ArgumentParser(description="Preprocess GeoTIFF/XLSX pairs into H5 patches with file-based train/val/test split.")
    parser.add_argument("--data_dir", type=str, default=str(BASE_DATA_DIR), help="Path to the root data directory.")
    parser.add_argument("--output_dir", type=str, default=str(PREPROCESSED_DATA_DIR), help="Path to the output directory for H5 files.")
    args = parser.parse_args()

    data_dir, output_dir = Path(args.data_dir), Path(args.output_dir)
    print("--- Starting Preprocessing ---")

    # 1. Find all potential data files
    all_data_pairs = find_data_pairs(data_dir)
    if not all_data_pairs: print("No data pairs found. Exiting."); return

    # 2. Identify files relevant to the ROI
    try:
        roi_poly_wgs84 = load_roi_polygon_wgs84(data_dir)
        relevant_files = get_roi_overlapping_files(all_data_pairs, roi_poly_wgs84)
        print(f"\nFound {len(relevant_files)} file pairs overlapping with the ROI.")
        if not relevant_files: print("No files overlap with ROI. Exiting."); return
    except Exception as e:
        print(f"Error during ROI processing: {e}. Aborting."); return

    # 3. Split files into train, validation, and test sets
    random.seed(RANDOM_SEED)
    random.shuffle(relevant_files)
    
    num_files = len(relevant_files)
    if num_files >= 3:
        test_files = relevant_files[0:2]
        val_files = relevant_files[2:3]
        train_files = relevant_files[3:]
    elif num_files == 2:
        test_files, val_files, train_files = relevant_files[0:1], relevant_files[1:2], []
    else: # num_files == 1
        test_files, val_files, train_files = [], relevant_files[0:1], []

    file_splits = {'train': train_files, 'val': val_files, 'test': test_files}
    print("\n--- Data Split Summary ---")
    for split, files in file_splits.items():
        print(f"  {split.capitalize()} set ({len(files)} files): {[p[0].name for p in files]}")
    print("--------------------------\n")

    # 4. Process each split
    generated_patches = {'train': [], 'val': [], 'test': []}
    for split, files in file_splits.items():
        if not files: continue
        print(f"--- Processing {split.upper()} Set ---")
        split_output_dir = output_dir / split
        
        # We need the ROI in the CRS of each specific image
        transformer_to_image_crs = Transformer.from_crs(WGS84_CRS, CRS.from_user_input(rasterio.open(files[0][0]).crs), always_xy=True)
        roi_poly_image_crs = Polygon([transformer_to_image_crs.transform(x, y) for x, y in roi_poly_wgs84.exterior.coords])
        
        for tif_path, xlsx_path in tqdm(files, desc=f"Processing {split} files"):
            process_data_pair(tif_path, xlsx_path, roi_poly_image_crs, split_output_dir, generated_patches[split])

    # 5. Create visualizations for each split
    print("\n--- Creating Visualizations ---")
    for split, patches in generated_patches.items():
        if not patches:
            print(f"No patches generated for '{split}' set, skipping visualization.")
            continue
        
        num_samples = min(NUM_VISUALIZATIONS_PER_SPLIT, len(patches))
        print(f"Generating {num_samples} visualizations for '{split}' set...")
        
        samples_to_visualize = random.sample(patches, num_samples)
        split_vis_dir = VISUALIZATION_DIR / split
        
        for patch_info in tqdm(samples_to_visualize, desc=f"Visualizing {split} samples"):
            create_visualization(patch_info, split_vis_dir)
        print(f"Visualizations for '{split}' set saved to {split_vis_dir}")

    print("\n--- Preprocessing Complete ---")

if __name__ == "__main__":
    main()
